{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@linuskohl/extracting-and-linking-ontology-terms-from-text-7806ae8d8189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all direct child terms for term “disease by infectious agent” from DOID ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pronto import Ontology\n",
    "# load the DOID ontology\n",
    "#doid = Ontology(\"http://purl.obolibrary.org/obo/doid.obo\")\n",
    "# # select a node by ID\n",
    "# root_node = doid['DOID:0050117'] # disease by infectious agent\n",
    "# # print all children (distance=1) without the node itself\n",
    "# for term in root.subclasses(distance=1, with_self=False).to_set():  \n",
    "#     print(term.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DiseaseOntology/HumanDiseaseOntology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the DOID ontology\n",
    "doid = Ontology(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/Bioinformatics/HumanDiseaseOntology/src/ontology/doid.obo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fungal infectious disease\n",
      "pelvic inflammatory disease\n",
      "ornithine translocase deficiency\n",
      "infective endocarditis\n",
      "osteomyelitis\n",
      "parasitic infectious disease\n",
      "bacterial infectious disease\n",
      "viral infectious disease\n"
     ]
    }
   ],
   "source": [
    "# select a node by ID\n",
    "root_node = doid['DOID:0050117'] # disease by infectious agent\n",
    "# print all children (distance=1) without the node itself\n",
    "for term in root_node.subclasses(distance=1, with_self=False).to_set():  \n",
    "    print(term.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12074 sha256=a13b0c10863a3882d87bccbe4cbcd7ec6e99289e3adfbb19929478e262aacec2\n",
      "  Stored in directory: /Users/patsnap/Library/Caches/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from pronto import Ontology\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOIDExtractorComponent(object):\n",
    "    # name of the component\n",
    "    name = \"doid_extractor\"\n",
    "\n",
    "    def __init__(self, nlp, label=\"DOID\"):\n",
    "        # label that is applied to the matches\n",
    "        self.label = label\n",
    "\n",
    "        # load ontology\n",
    "        print(\"Loading DOID ontology\")\n",
    "        doid = Ontology(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/Bioinformatics/HumanDiseaseOntology/src/ontology/doid.obo\")\n",
    "        \n",
    "        # init terms and patterns\n",
    "        self.terms = {}\n",
    "        patterns = []\n",
    "\n",
    "        i = 0\n",
    "        nr_terms = len(doid.terms())\n",
    "        # init progress bar as loading terms takes long\n",
    "        print(\"Importing terms\")\n",
    "        bar = progressbar.ProgressBar(maxval=nr_terms, \n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        bar.start()\n",
    "\n",
    "        # iterate over terms in ontology\n",
    "        for term in doid.terms():\n",
    "          # if term has a name\n",
    "          if term.name is not None:\n",
    "            self.terms[term.name.lower()] = {'id': term.id}\n",
    "            patterns.append(nlp(term.name))\n",
    "          i += 1\n",
    "          bar.update(i)\n",
    "            \n",
    "        bar.finish()\n",
    "        \n",
    "        # initialize matcher and add patterns\n",
    "        self.matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "        \n",
    "        # set extensions to tokens, spans and docs\n",
    "        Token.set_extension(\"is_doid_term\", default=False, force=True)\n",
    "        Token.set_extension(\"doid_id\", default=False, force=True)\n",
    "        Token.set_extension(\"merged_concept\", default=False, force=True)\n",
    "        Doc.set_extension(\"has_doids\", getter=self.has_doids, force=True)\n",
    "        Doc.set_extension(\"doids\", default=[], force=True)\n",
    "        Span.set_extension(\"has_doids\", getter=self.has_doids, force=True)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = [Span(doc, match[1], match[2], label=self.label) for match in matches]\n",
    "        for i, span in enumerate(spans):\n",
    "          span._.set(\"has_doids\", True)\n",
    "          for token in span:\n",
    "               token._.set(\"is_doid_term\", True)\n",
    "               token._.set(\"doid_id\", self.terms[span.text.lower()][\"id\"])\n",
    "\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in filter_spans(spans):\n",
    "                retokenizer.merge(span, attrs={\"_\": {\"merged_concept\": True}})\n",
    "                doc._.doids = list(doc._.doids) + [span]\n",
    "\n",
    "        return doc\n",
    "    # setter function for doc level\n",
    "    def has_doids(self, tokens):\n",
    "        return any([t._.get(\"is_doid_term\") for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pronto import Ontology\n",
    "import progressbar\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DOID ontology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "doid_extractor = DOIDExtractorComponent(nlp)\n",
    "nlp.add_pipe(doid_extractor, after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample sentences from the publication:\n",
    "# \"Pulmonary and Cardiac Pathology in Covid-19: The First Autopsy Series from New Orleans\", Sharon E. Fox et.al.\n",
    "test = \"\"\"\n",
    "Whether this may represent an early manifestation of a viral myocarditis is not certain,\n",
    "but there was no significant brisk lymphocytic inflammatory infiltrate consistent with the\n",
    "typical pattern of viral myocarditis...\n",
    "There is prior evidence of viral infection causing activation of both maladaptive cytokine pathways,\n",
    "and platelet response, and our findings suggest that these immune functions may be related to\n",
    "severe forms of Covid-19. In response to systemic and pulmonary viral infections of H1N1\n",
    "influenza and dengue, megakaryocytes have been known to respond by overexpressing IFITM3,\n",
    "and producing platelets with the same over-expression.\n",
    "\"\"\"\n",
    "doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myocarditis\n",
      "http://purl.obolibrary.org/obo/DOID_820\t\tmyocarditis\n",
      "myocarditis\n",
      "http://purl.obolibrary.org/obo/DOID_820\t\tmyocarditis\n",
      "Covid-19\n",
      "http://purl.obolibrary.org/obo/DOID_0080600\t\tCovid-19\n",
      "influenza\n",
      "http://purl.obolibrary.org/obo/DOID_8469\t\tinfluenza\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token._.is_doid_term:\n",
    "        print(token)\n",
    "        print(\"http://purl.obolibrary.org/obo/{}\\t\\t{}\".format(token._.doid_id.replace(\":\",\"_\"), token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
